{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SharePoint to Fabric Lakehouse ETL\n",
    "\n",
    "## Overview\n",
    "This notebook automates the process of downloading Excel files from SharePoint and loading them into Microsoft Fabric Lakehouse.\n",
    "\n",
    "### What it does:\n",
    "- Downloads Excel files from SharePoint using sharing links\n",
    "- Writes files to Fabric Lakehouse\n",
    "- Overwrites existing files on each run\n",
    "- Provides detailed logging and error reporting\n",
    "\n",
    "### Prerequisites:\n",
    "- Microsoft Fabric workspace with a Lakehouse\n",
    "- SharePoint files with \"Anyone with link can edit\" sharing enabled\n",
    "- Proper permissions to write to the Lakehouse\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Setup Instructions\n",
    "\n",
    "### Step 1: Get SharePoint Sharing Links\n",
    "For each Excel file you want to sync:\n",
    "1. Navigate to the file in SharePoint\n",
    "2. Right-click the file ‚Üí **Share**\n",
    "3. Select **\"Anyone with the link can edit\"**\n",
    "4. Click **Copy link**\n",
    "5. Save this link - you'll need it in the configuration below\n",
    "\n",
    "### Step 2: Get Your Lakehouse Path\n",
    "1. In Fabric, open your Lakehouse\n",
    "2. Click on the **Files** folder\n",
    "3. Copy the ABFS path (format: `abfss://workspace@onelake.dfs.fabric.microsoft.com/lakehouse.Lakehouse/Files/folder`)\n",
    "\n",
    "### Step 3: Configure the notebook\n",
    "Update the configuration in **Cell 4** below with your values.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install requests --quiet\n",
    "print(\"‚úì Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from datetime import datetime\n",
    "from notebookutils import mssparkutils\n",
    "from urllib.parse import unquote\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuration Section\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANT: Update all values below before running the notebook**\n",
    "\n",
    "Replace the placeholder values with your actual:\n",
    "- Lakehouse ABFS path\n",
    "- SharePoint tenant and site names\n",
    "- File URLs and sharing links\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION - UPDATE THESE VALUES\n",
    "# ============================================================================\n",
    "\n",
    "# Lakehouse Configuration\n",
    "# Format: abfss://WORKSPACE_NAME@onelake.dfs.fabric.microsoft.com/LAKEHOUSE_NAME.Lakehouse/Files/FOLDER_PATH\n",
    "lakehouse_abfs_path = \"abfss://YOUR_WORKSPACE@onelake.dfs.fabric.microsoft.com/YOUR_LAKEHOUSE.Lakehouse/Files/YOUR_FOLDER\"\n",
    "\n",
    "# SharePoint Configuration\n",
    "sharepoint_tenant = \"your-tenant-name\"  # e.g., \"contoso\"\n",
    "sharepoint_site = \"your-site-name\"      # e.g., \"finance\" or \"projects\"\n",
    "\n",
    "# Source Files Configuration\n",
    "# For each file, provide:\n",
    "# 1. Original SharePoint URL (for reference)\n",
    "# 2. Sharing link (Right-click file > Share > \"Anyone with link can edit\" > Copy link)\n",
    "# 3. Desired filename in lakehouse\n",
    "# 4. Description (for logging purposes)\n",
    "\n",
    "source_files = [\n",
    "    {\n",
    "        \"url\": \"https://YOUR_TENANT.sharepoint.com/sites/YOUR_SITE/Shared%20Documents/Path/To/File1.xlsx\",\n",
    "        \"sharing_link\": \"\",  # PASTE YOUR SHARING LINK HERE\n",
    "        \"lakehouse_name\": \"File1.xlsx\",\n",
    "        \"description\": \"First Excel File\"\n",
    "    },\n",
    "    {\n",
    "        \"url\": \"https://YOUR_TENANT.sharepoint.com/sites/YOUR_SITE/Shared%20Documents/Path/To/File2.xlsx\",\n",
    "        \"sharing_link\": \"\",  # PASTE YOUR SHARING LINK HERE\n",
    "        \"lakehouse_name\": \"File2.xlsx\",\n",
    "        \"description\": \"Second Excel File\"\n",
    "    }\n",
    "    # Add more files as needed following the same pattern\n",
    "]\n",
    "\n",
    "# ============================================================================\n",
    "# END CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"‚úì Configuration loaded\")\n",
    "print(f\"  Target Lakehouse: {lakehouse_abfs_path}\")\n",
    "print(f\"  Files to process: {len(source_files)}\")\n",
    "print(\"\\nSharing links status:\")\n",
    "for file in source_files:\n",
    "    status = \"‚úì\" if file.get('sharing_link') else \"‚úó MISSING\"\n",
    "    print(f\"  {status} {file['lakehouse_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Core Functions\n",
    "\n",
    "The following cells define the ETL functions. **No changes needed in this section.**\n",
    "\n",
    "### Functions:\n",
    "1. **download_from_sharepoint()** - Downloads files using sharing links\n",
    "2. **write_to_lakehouse()** - Writes files to Lakehouse with overwrite\n",
    "3. **run_etl()** - Main orchestration function\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_from_sharepoint(file_config):\n",
    "    \"\"\"Download Excel file from SharePoint using sharing link\"\"\"\n",
    "    try:\n",
    "        sharing_link = file_config.get('sharing_link', '')\n",
    "        \n",
    "        if not sharing_link:\n",
    "            print(\"  ‚úó No sharing link configured\")\n",
    "            return None\n",
    "        \n",
    "        # Convert sharing link to direct download URL\n",
    "        if 'sharepoint.com' in sharing_link:\n",
    "            base_url = sharing_link.split('?')[0]\n",
    "            download_url = base_url + '?download=1'\n",
    "        else:\n",
    "            download_url = sharing_link\n",
    "        \n",
    "        print(\"  Downloading from SharePoint...\")\n",
    "        response = requests.get(download_url, allow_redirects=True, timeout=60)\n",
    "        \n",
    "        # Verify successful download\n",
    "        if response.status_code == 200 and len(response.content) > 5000:\n",
    "            # Check if content is Excel file (ZIP format)\n",
    "            if response.content[:2] == b'PK':\n",
    "                print(f\"  ‚úì Downloaded successfully ({len(response.content):,} bytes)\")\n",
    "                return BytesIO(response.content)\n",
    "            else:\n",
    "                print(\"  ‚úó Content is not an Excel file\")\n",
    "        else:\n",
    "            print(f\"  ‚úó Download failed: Status {response.status_code}\")\n",
    "        \n",
    "        return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚úó Error: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_lakehouse(file_bytes, lakehouse_path, filename):\n",
    "    \"\"\"Write file to Fabric Lakehouse (overwrites if exists)\"\"\"\n",
    "    try:\n",
    "        full_path = f\"{lakehouse_path}/{filename}\"\n",
    "        file_data = file_bytes.getvalue()\n",
    "        \n",
    "        # Write to temporary local file\n",
    "        temp_path = f\"/tmp/{filename}\"\n",
    "        with open(temp_path, 'wb') as f:\n",
    "            f.write(file_data)\n",
    "        \n",
    "        print(f\"  ‚úì Temporary file created ({len(file_data):,} bytes)\")\n",
    "        \n",
    "        # Remove existing file if it exists\n",
    "        try:\n",
    "            mssparkutils.fs.rm(full_path)\n",
    "            print(\"  ‚úì Existing file removed\")\n",
    "        except:\n",
    "            print(\"  No existing file to remove\")\n",
    "        \n",
    "        # Copy to lakehouse\n",
    "        print(\"  Writing to lakehouse...\")\n",
    "        mssparkutils.fs.cp(f\"file://{temp_path}\", full_path)\n",
    "        \n",
    "        # Clean up temporary file\n",
    "        os.remove(temp_path)\n",
    "        \n",
    "        print(f\"  ‚úì Successfully written to lakehouse\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚úó Error writing to lakehouse: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_etl():\n",
    "    \"\"\"Main ETL process - Downloads files from SharePoint and writes to Lakehouse\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Starting ETL Process - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    success_count = 0\n",
    "    failed_count = 0\n",
    "    results = []\n",
    "    \n",
    "    for idx, file_config in enumerate(source_files, 1):\n",
    "        print(f\"\\n[{idx}/{len(source_files)}] Processing: {file_config['description']}\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"Target: {file_config['lakehouse_name']}\")\n",
    "        print()\n",
    "        \n",
    "        # Download from SharePoint\n",
    "        file_bytes = download_from_sharepoint(file_config)\n",
    "        \n",
    "        if file_bytes:\n",
    "            # Write to Lakehouse\n",
    "            success = write_to_lakehouse(\n",
    "                file_bytes,\n",
    "                lakehouse_abfs_path,\n",
    "                file_config['lakehouse_name']\n",
    "            )\n",
    "            \n",
    "            if success:\n",
    "                success_count += 1\n",
    "                results.append({\n",
    "                    'file': file_config['lakehouse_name'],\n",
    "                    'status': 'SUCCESS',\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                })\n",
    "                print(f\"\\n‚úì {file_config['lakehouse_name']} - COMPLETED SUCCESSFULLY\")\n",
    "            else:\n",
    "                failed_count += 1\n",
    "                results.append({\n",
    "                    'file': file_config['lakehouse_name'],\n",
    "                    'status': 'FAILED - Write Error',\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                })\n",
    "                print(f\"\\n‚úó {file_config['lakehouse_name']} - WRITE FAILED\")\n",
    "        else:\n",
    "            failed_count += 1\n",
    "            results.append({\n",
    "                'file': file_config['lakehouse_name'],\n",
    "                'status': 'FAILED - Download Error',\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            })\n",
    "            print(f\"\\n‚úó {file_config['lakehouse_name']} - DOWNLOAD FAILED\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"ETL Process Complete - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"‚úì Success: {success_count}\")\n",
    "    print(f\"‚úó Failed:  {failed_count}\")\n",
    "    print(\"\\nDetailed Results:\")\n",
    "    for result in results:\n",
    "        status_icon = \"‚úì\" if result['status'] == 'SUCCESS' else \"‚úó\"\n",
    "        print(f\"  {status_icon} {result['file']}: {result['status']}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return success_count, failed_count, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ñ∂Ô∏è Execute ETL\n",
    "\n",
    "Run the cell below to start the ETL process.\n",
    "\n",
    "**What happens:**\n",
    "1. Downloads each file from SharePoint\n",
    "2. Writes to Lakehouse (overwrites if exists)\n",
    "3. Displays progress and results\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the ETL process\n",
    "success, failed, results = run_etl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification\n",
    "\n",
    "Run the cell below to verify files were successfully written to the Lakehouse.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify files were written to lakehouse\n",
    "print(\"\\nFiles in Lakehouse:\")\n",
    "print(\"=\" * 80)\n",
    "try:\n",
    "    files = mssparkutils.fs.ls(lakehouse_abfs_path)\n",
    "    for file in files:\n",
    "        print(f\"  üìÑ {file.name} ({file.size:,} bytes)\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not list files: {str(e)}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÅ Completion\n",
    "\n",
    "The cell below exits the notebook with the appropriate status code.\n",
    "\n",
    "**Exit Codes:**\n",
    "- Success: All files processed successfully\n",
    "- Partial: Some files failed\n",
    "- ‚úó Failure: All files failed\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exit notebook with appropriate status\n",
    "if failed > 0:\n",
    "    exit_msg = f\"ETL completed with {failed} failures and {success} successes\"\n",
    "    print(f\"\\n‚ö†Ô∏è  {exit_msg}\")\n",
    "    mssparkutils.notebook.exit(exit_msg)\n",
    "else:\n",
    "    exit_msg = f\"ETL completed successfully. All {success} files processed.\"\n",
    "    print(f\"\\n‚úì {exit_msg}\")\n",
    "    mssparkutils.notebook.exit(exit_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Troubleshooting\n",
    "\n",
    "### Common Issues:\n",
    "\n",
    "**‚ùå \"No sharing link configured\"**\n",
    "- Ensure you've added the sharing link in Cell 6 configuration\n",
    "\n",
    "**‚ùå \"Download failed: Status 403\"**\n",
    "- Sharing link permissions may be restricted\n",
    "- Verify \"Anyone with link can edit\" is enabled\n",
    "\n",
    "**‚ùå \"Download failed: Status 404\"**\n",
    "- File not found or sharing link expired\n",
    "- Regenerate the sharing link\n",
    "\n",
    "**‚ùå \"Content is not an Excel file\"**\n",
    "- Verify the file is actually an Excel file (.xlsx)\n",
    "- Check if the sharing link points to the correct file\n",
    "\n",
    "**‚ùå \"Error writing to lakehouse\"**\n",
    "- Verify lakehouse path is correct\n",
    "- Check workspace permissions\n",
    "\n",
    "---\n",
    "\n",
    "## üìÖ Scheduling\n",
    "\n",
    "To run this notebook automatically:\n",
    "1. Create a Fabric Pipeline\n",
    "2. Add a Notebook activity\n",
    "3. Select this notebook\n",
    "4. Configure a Schedule trigger (daily, hourly, etc.)\n",
    "5. Save and activate\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Notes\n",
    "\n",
    "- Files are **overwritten** on each run\n",
    "- Process continues even if individual files fail\n",
    "- Check the summary for detailed status of each file"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
